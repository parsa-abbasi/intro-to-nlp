{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMyiIt8n1DeQ47Lrm0nX4Ky",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/parsa-abbasi/intro-to-nlp/blob/main/NLP_text_representation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## One-hot Encoding\n",
        "\n",
        "One-hot encoding is a representation method that represents each word as a vector of 0s and 1s. The length of the vector is equal to the size of the vocabulary. Each word is represented by a vector that has a 1 in the position that corresponds to the index of the word in the vocabulary and 0s in all other positions."
      ],
      "metadata": {
        "id": "6YVXtCqRujPP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pp6_wIQ1vwDH",
        "outputId": "76ceea16-6fff-47d6-a30f-d876329e00aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "text = \"He who thinks great thoughts often makes great errors\"\n",
        "\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "one_hot_encoder = OneHotEncoder(sparse_output=False)\n",
        "\n",
        "one_hot_encoded = one_hot_encoder.fit_transform([[token] for token in tokens])\n",
        "\n",
        "print(tokens)\n",
        "print(one_hot_encoded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0pMPlOdWvf1i",
        "outputId": "5fc3efbf-7a72-451e-e39c-927378d52df3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['He', 'who', 'thinks', 'great', 'thoughts', 'often', 'makes', 'great', 'errors']\n",
            "[[1. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bag-of-Words (BoW)\n",
        "\n",
        "Bag-of-Words (BoW) is a representation method that represents each document as a vector of numbers. The length of the vector is equal to the size of the vocabulary. Each document is represented by a vector that has the count of each word in the vocabulary.\n",
        "\n",
        "It is called a “bag” of words, because any information about the order or structure of words in the document is discarded."
      ],
      "metadata": {
        "id": "l98n9EIo3dqj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "docs = [\"He who thinks great thoughts often makes great errors\",\n",
        "        \"The most thought-provoking thing in our thought-provoking time is that we are still not thinking\"]\n",
        "\n",
        "vectorizer = CountVectorizer(lowercase=True, tokenizer=word_tokenize, stop_words=['the'])\n",
        "\n",
        "bow = vectorizer.fit_transform(docs)\n",
        "\n",
        "print(vectorizer.get_feature_names_out())\n",
        "print(bow.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "06d9lMwPvpML",
        "outputId": "b5b83959-e0f8-41f8-9eb3-99160f469e00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['are' 'errors' 'great' 'he' 'in' 'is' 'makes' 'most' 'not' 'often' 'our'\n",
            " 'still' 'that' 'thing' 'thinking' 'thinks' 'thought-provoking' 'thoughts'\n",
            " 'time' 'we' 'who']\n",
            "[[0 1 2 1 0 0 1 0 0 1 0 0 0 0 0 1 0 1 0 0 1]\n",
            " [1 0 0 0 1 1 0 1 1 0 1 1 1 1 1 0 2 0 1 1 0]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tf-idf"
      ],
      "metadata": {
        "id": "EuIBwWZBAd9n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "docs = [\"He who thinks great thoughts often makes great errors\",\n",
        "        \"The most thought-provoking thing in our thought-provoking time is that we are still not thinking\"]\n",
        "\n",
        "vectorizer = TfidfVectorizer(lowercase=True, tokenizer=word_tokenize)\n",
        "\n",
        "tf_idf = vectorizer.fit_transform(docs)\n",
        "\n",
        "print(vectorizer.get_feature_names_out())\n",
        "print(tf_idf.shape)\n",
        "print(tf_idf.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FDBgaaRQ3j8j",
        "outputId": "7a459379-6c3f-4cf8-87e7-294ba4d0deff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['are' 'errors' 'great' 'he' 'in' 'is' 'makes' 'most' 'not' 'often' 'our'\n",
            " 'still' 'that' 'the' 'thing' 'thinking' 'thinks' 'thought-provoking'\n",
            " 'thoughts' 'time' 'we' 'who']\n",
            "(2, 22)\n",
            "[[0.         0.30151134 0.60302269 0.30151134 0.         0.\n",
            "  0.30151134 0.         0.         0.30151134 0.         0.\n",
            "  0.         0.         0.         0.         0.30151134 0.\n",
            "  0.30151134 0.         0.         0.30151134]\n",
            " [0.24253563 0.         0.         0.         0.24253563 0.24253563\n",
            "  0.         0.24253563 0.24253563 0.         0.24253563 0.24253563\n",
            "  0.24253563 0.24253563 0.24253563 0.24253563 0.         0.48507125\n",
            "  0.         0.24253563 0.24253563 0.        ]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3tkqE1RFBFxI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}